{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# riboswitch and ribosnitch strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Figure_2b ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy import io as sio \n",
    "from utils_io import filter_data_by_modrate \n",
    "from dreem import DREEM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1_1. get the input reads ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sizes(f_sizes):\n",
    "    sizes = {}\n",
    "    with open(f_sizes, \"r\") as f:\n",
    "        for line in f:\n",
    "            row = line.strip(\"\\r\\n\").split(\"\\t\")\n",
    "            sizes[row[0]] = int(row[1])\n",
    "    return sizes \n",
    "\n",
    "def convert_mm_to_df(f_mtx, f_id, p_gene, \n",
    "                     f_sizes, p_sep=\",\", p_threshold_len_prop=0.8):\n",
    "    \"\"\"\n",
    "    Utility function for processing sparse matrix .mtx files\n",
    "    \"\"\"\n",
    "\n",
    "    # load sizes file\n",
    "    sizes = get_sizes(f_sizes)\n",
    "    \n",
    "    # load iid file\n",
    "    if pd.read_csv(f_id, index_col=0, sep=p_sep).shape[1] == 1:\n",
    "        df_iid = pd.read_csv(f_id, index_col=0, sep=p_sep, names=[\"iid\"])\n",
    "    else:\n",
    "        df_iid = pd.read_csv(f_id, index_col=0, sep=p_sep, names=[\"iid\", \"length\", \"start\"])\n",
    "    \n",
    "    # load sparse matrix\n",
    "    sm = sio.mmread(f_mtx)\n",
    "\n",
    "    # determine whether sparse matrix is undersized and remedy if so\n",
    "    new_row_size = df_iid.shape[0]\n",
    "    new_column_size = sizes[p_gene]\n",
    "    if sm.shape[1] < new_column_size:  # resize\n",
    "        sm.resize((new_row_size, new_column_size))\n",
    "    elif sm.shape[1] > new_column_size:\n",
    "        print(sm.shape[1], new_column_size)\n",
    "        print(\"shapemap column greater than genome sizes file\")\n",
    "        assert False\n",
    "\n",
    "    # load sparse matrix as a pandas dataframe\n",
    "    df_mm = pd.DataFrame.sparse.from_spmatrix(sm)\n",
    "    df_mm.index = df_iid.iid\n",
    "\n",
    "    if df_iid.shape[1] >= 2:\n",
    "        # Threshold by Length\n",
    "        df_mm = df_mm.loc[df_iid[df_iid.length >= (p_threshold_len_prop * sizes[p_gene])].iid,:]\n",
    "        #df_mm = df_mm.loc[df_iid[df_iid.length >= 300].iid,:]\n",
    "    \n",
    "    return df_mm\n",
    "\n",
    "\n",
    "def load_data(f_mtxs, f_ids, p_genes, f_sizes=None, p_depth=-1, p_length=-1, \n",
    "              p_start=None, p_end=None, p_threshold=1,\n",
    "              p_threshold_len_prop=0.8, p_verbose=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads SHAPE-MaP or PORE-cupine data based on the input specifications\n",
    "    Must be able to accept both dense matrix or sparse matrix\n",
    "    \"\"\"\n",
    "\n",
    "    dfs = []\n",
    "    for f_mtx, f_id, p_gene in zip(f_mtxs, f_ids, p_genes):\n",
    "        df = convert_mm_to_df(f_mtx, f_id, p_gene,\n",
    "                            f_sizes=f_sizes,\n",
    "                            p_threshold_len_prop=p_threshold_len_prop)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    \n",
    "    if p_length == -1:\n",
    "        p_start = 0\n",
    "        p_end = dfs[0].shape[1]\n",
    "\n",
    "    if p_depth == -1:\n",
    "        p_depth = min([ df.shape[0] for df in dfs ])\n",
    "\n",
    "    if p_verbose:\n",
    "        print(\"%s %s %s\" % (p_start, p_end, \" \".join(map(str, [ df.shape[0] for df in dfs]))))\n",
    "        print(p_depth)\n",
    "\n",
    "    if p_depth is None:\n",
    "        ndfs =  [ df.iloc[:,p_start:p_end] for df in dfs ]\n",
    "    else:\n",
    "        ndfs = [ df.iloc[:p_depth,p_start:p_end] for df in dfs ]\n",
    "    \n",
    "    ndf = pd.concat(ndfs, axis=0)\n",
    "\n",
    "    X = ndf\n",
    "    X = X.loc[X.sum(axis=1)>=p_threshold,:]\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    return X, ndfs\n",
    "\n",
    "\n",
    "def convert_to_dreem(sid,seq,p_length,f_output):\n",
    "        with open(f_output, \"w\") as f:\n",
    "            f.write(\"\\t\".join([\"@ref\", \"%s;%s\" % (sid, sid),seq]) + \"\\n\")\n",
    "            f.write(\"\\t\".join([\"@coordinates:length\", \"1,%s:%s\" % (p_length,p_length)]) + \"\\n\")\n",
    "            f.write(\"\\t\".join([\"Query_name\", \"Bit_vector\", \"N_Mutations\"]) + \"\\n\")\n",
    "            for no, d in enumerate(library):\n",
    "                no_of_mutations = d.sum()\n",
    "                f.write(\"\\t\".join([\"%s.1\" % no,\n",
    "                                \"\".join(map(str, d)),\n",
    "                                str(no_of_mutations)]) + \"\\n\")\n",
    "        return True \n",
    "\n",
    "def write_to_fasta(sid,seq,f_output):\n",
    "        with open(f_output, \"w\") as f:\n",
    "            f.write(\">%s\\n\" % sid)\n",
    "            f.write(\"%s\\n\" % seq)\n",
    "        return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 198 148814\n",
      "None\n",
      "0 198 349789\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/users/hanjian/rerun_analysis/0_Manuscript_codes_submission_20250723/Figure2/utils_io.py:229: FutureWarning:\n",
      "\n",
      "In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111750, 198) (229784, 198)\n",
      "(2000, 198)\n",
      "198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_genes = [\"ADD-TRUNC\"]\n",
    "p_mod = \"NAIN3\"\n",
    "p_depth = None\n",
    "p_length = -1\n",
    "p_start = None\n",
    "p_end = None\n",
    "p_threshold_len_prop = 0.85\n",
    "p_verbose = True\n",
    "\n",
    "f_sizes = \"/home/han/proj_het_AC/rerun_analysis/0_Manuscript_codes_submission_20250723/Figure2/Data/reference_of_ribosxitch_v2.sizes\"\n",
    "\n",
    "### WT ----\n",
    "f_dir = \"/home/han/proj_het_AC/rerun_analysis/0_Manuscript_codes_submission_20250723/Figure2/Data/DMS-MaP/\"\n",
    "f_mtxs = [f_dir + \"NOADD_DMS-MaP_DMS_B01_raw.mtx\"]\n",
    "f_ids =  [ f_dir + \"NOADD_DMS-MaP_DMS_B01_raw.iids.gz\"]\n",
    "\n",
    "\n",
    "X_WT, df_WTs  = load_data(f_mtxs, f_ids, p_genes, f_sizes=f_sizes, \n",
    "                                  p_depth=p_depth, p_length=p_length, \n",
    "                                  p_start=p_start, p_end=p_end, \n",
    "                                  p_threshold_len_prop=p_threshold_len_prop, \n",
    "                                  p_verbose=p_verbose)\n",
    "\n",
    "### MT ----\n",
    "f_dir = \"/home/han/proj_het_AC/rerun_analysis/0_Manuscript_codes_submission_20250723/Figure2/Data/DMS-MaP/\"\n",
    "f_mtxs = [f_dir + \"ADD_DMS-MaP_DMS_B01_raw.mtx\"]\n",
    "f_ids =  [f_dir + \"ADD_DMS-MaP_DMS_B01_raw.iids.gz\"]\n",
    "\n",
    "X_MT, df_MTs  = load_data(f_mtxs, f_ids, p_genes, f_sizes=f_sizes, \n",
    "                                  p_depth=p_depth, p_length=p_length, \n",
    "                                  p_start=p_start, p_end=p_end, \n",
    "                                  p_threshold_len_prop=p_threshold_len_prop, \n",
    "                                  p_verbose=p_verbose)\n",
    "\n",
    "df_WT = pd.concat(df_WTs)\n",
    "df_MT = pd.concat(df_MTs)\n",
    "X = pd.concat([X_WT, X_MT])\n",
    "\n",
    "p_threshold_len_prop=0.85\n",
    "p_modrate_low_pos=0.0\n",
    "p_modrate_high_pos=1.0\n",
    "p_modrate_low_read=0.0075\n",
    "p_modrate_high_read=0.025\n",
    "p_min_samples = 1000\n",
    "p_threshold = 0.5\n",
    "\n",
    "# Filter reads and positions by min/max modification rate\n",
    "df_WT_flt = filter_data_by_modrate(df_WT, \n",
    "                                    p_modrate_low=p_modrate_low_pos, \n",
    "                                    p_modrate_high=p_modrate_high_pos, p_axis=0)  # this will zero the data instead of remove\n",
    "df_MT_flt = filter_data_by_modrate(df_MT, \n",
    "                                    p_modrate_low=p_modrate_low_pos, \n",
    "                                    p_modrate_high=p_modrate_high_pos, p_axis=0)  # this will zero the data instead of remove\n",
    "\n",
    "df_WT_flt = filter_data_by_modrate(df_WT_flt, p_modrate_low=p_modrate_low_read, p_modrate_high=p_modrate_high_read, p_axis=1)\n",
    "df_MT_flt = filter_data_by_modrate(df_MT_flt, p_modrate_low=p_modrate_low_read, p_modrate_high=p_modrate_high_read, p_axis=1)\n",
    "\n",
    "print(df_WT_flt.shape, df_MT_flt.shape)\n",
    "\n",
    "\n",
    "p_min_samples = 1000\n",
    "\n",
    "# BMM\n",
    "np.random.seed(386) #386, 472, 123, 823\n",
    "    \n",
    "if p_min_samples == -1:\n",
    "    min_samples = min(df_WT_flt.shape[0], df_MT_flt.shape[0]) #2900\n",
    "else:\n",
    "    min_samples = p_min_samples\n",
    "    \n",
    "total_reads = p_min_samples*2\n",
    "prop=0.5\n",
    "WT_reads = total_reads*prop\n",
    "MT_reads = total_reads-WT_reads\n",
    "df_WT_flt2 = df_WT_flt.sample(int(WT_reads), random_state=386)\n",
    "df_MT_flt2 = df_MT_flt.sample(int(MT_reads), random_state=386)\n",
    "\n",
    "new_X = pd.concat([df_WT_flt2, df_MT_flt2])\n",
    "\n",
    "df_WT_flt2['group'] = 'Noligand'\n",
    "df_MT_flt2['group'] = 'Ligand'\n",
    "new_X2 = pd.concat([df_WT_flt2, df_MT_flt2])\n",
    "print(new_X.shape)\n",
    "\n",
    "\n",
    "truths = [0]*1000 + [1]*1000\n",
    "data =np.array(new_X)\n",
    "sid = 'ADD'\n",
    "seq = 'GGACACGACTCGAGTAGAGTCGAATGCGCTTCATATAATCCTAATGATATGGTTTGGGAGTTTCTACCAAGAGCCTTAAACTCTTGATTATGAAGTCTGTCGCTTTATCCGAAATTTTATAAAGAGAAGACTCATGAATTACTTTGACCTGCCGACCGGAGTCGAGTAGACTCCAACAAAAGAAACAACAACAACAAC'\n",
    "p_length = 198\n",
    "library= data \n",
    "print(len(seq)) \n",
    "dreem_data=convert_to_dreem(sid,seq,p_length,\"/home/han/proj_het_AC/rerun_analysis/0_Manuscript_codes_submission_20250723/Figure2/Data/DMS-MaP/result/input_dreem.txt\")\n",
    "write_to_fasta(sid,seq,\"/home/han/proj_het_AC/rerun_analysis/0_Manuscript_codes_submission_20250723/Figure2/Data/DMS-MaP/result/input_fasta.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1_2. run DREEM model ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutations threshold: 20\n",
      "Total bit vectors: 2000\n",
      "Bit vectors removed because of too many mutations:  0\n",
      "Bit vectors removed because of too few informative bits:  0\n",
      "Bit vectors removed because of mutations close by:  0\n",
      "Bit vectors removed because of no info around mutations:  0\n",
      "Working on K = 2\n",
      "Run number: 1\n",
      "3 -23571.016549024385 46.429388306143665 -0.001969766056104415\n",
      "4 -23533.16860468245 37.847944341934635 -0.0016082808472465515\n",
      "5 -23490.59268111755 42.57592356490204 -0.0018124669795635195\n",
      "6 -23444.56002038139 46.032660736160324 -0.0019634687405582405\n",
      "7 -23402.449262439688 42.110757941700285 -0.001799416696494539\n",
      "8 -23370.106458266728 32.342804172960314 -0.001383939103175102\n",
      "9 -23348.127165631406 21.979292635322054 -0.0009413728338637676\n",
      "10 -23334.598164635063 13.529000996342802 -0.0005797829000906812\n",
      "11 -23326.61323248457 7.984932150491659 -0.0003423099646275211\n",
      "12 -23322.00418043111 4.609052053459891 -0.00019762675702319043\n",
      "13 -23319.286609908973 2.717570522137976 -0.0001165374639283866\n",
      "14 -23317.581649030915 1.7049608780580456 -7.311911259583407e-05\n",
      "15 -23316.42353992063 1.1581091102852952 -4.9669243153971176e-05\n",
      "16 -23315.566233399873 0.8573065207565378 -3.6769706220063155e-05\n",
      "17 -23314.87396628277 0.6922671171050752 -2.9692080605119716e-05\n",
      "18 -23314.271292710746 0.6026735720224679 -2.5849985378307534e-05\n",
      "19 -23313.720654723915 0.5506379868311342 -2.361862334142542e-05\n",
      "20 -23313.206648581065 0.5140061428501213 -2.204785255834404e-05\n",
      "21 -23312.71764958437 0.4889989966941357 -2.0975632444244604e-05\n",
      "Log like converged after 21 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/users/hanjian/miniconda3/envs/gis/lib/python3.9/site-packages/plotly/tools.py:461: DeprecationWarning:\n",
      "\n",
      "plotly.tools.make_subplots is deprecated, please use plotly.subplots.make_subplots instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dreem.DREEM at 0x7f60cdee9070>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_dreem_prefix = \"/home/han/proj_het_AC/rerun_analysis/0_Manuscript_codes_submission_20250723/Figure2/Data/DMS-MaP/result/\"\n",
    "dir_input = f_dreem_prefix + \"input/\"\n",
    "os.system(\"mkdir -p %s\" % dir_input)\n",
    "dir_output = \"./\"\n",
    "dir_outplot = f_dreem_prefix + (\"output/%s/\" % sid)\n",
    "os.system(\"mkdir -p %s\" % dir_outplot)\n",
    "\n",
    "p_cluster=2\n",
    "p_seed = 386\n",
    "model = DREEM(sid, \n",
    "              data, \n",
    "              dir_input, dir_output, dir_outplot,\n",
    "              MIN_ITS=3, INFO_THRESH=0.05, CONV_CUTOFF=0.5, NUM_RUNS=1,  # 1e-5\n",
    "              MAX_K=p_cluster, SIG_THRESH=0.001, BV_THRESH=0, \n",
    "              NORM_PERC_BASES=10, inc_TG=True, p_seed=p_seed)\n",
    "\n",
    "# Fit model and Parse DREEM results into DREEM object\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1_3. process DREEM result ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dreem_results(f_result, p_cluster, library, truths, p_threshold=0.5):\n",
    "        datum_pred = []\n",
    "        dict_pred = {}\n",
    "        with open(f_result, \"r\") as f:\n",
    "            for no, line in enumerate(f):\n",
    "                if no % 2 == 1:\n",
    "                    row = line.strip(\"\\r\\n\").split(\"\\t\")\n",
    "                    rid, posteriors, bitvector = row[0], list(map(float, row[1:1+p_cluster])), row[-1]\n",
    "                    cluster_no = np.argmax(posteriors)  # max posterior\n",
    "                    datum_pred.append(cluster_no)\n",
    "                    dict_pred[bitvector] = cluster_no\n",
    "\n",
    "        new_datum_truth = []\n",
    "        new_datum_pred = []\n",
    "        new_data = []\n",
    "        new_index = []\n",
    "        for no, (d, t) in enumerate(zip(library, truths)):\n",
    "            try:\n",
    "                p = dict_pred[\"\".join(map(str, d.tolist()))]\n",
    "                new_datum_truth.append(t)\n",
    "                new_datum_pred.append(p)\n",
    "                new_data.append(d)\n",
    "                new_index.append(no)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        new_datum_truth = np.array(new_datum_truth)\n",
    "        new_datum_pred = np.array(new_datum_pred)\n",
    "        new_data = np.array(new_data)\n",
    "        new_index = np.array(new_index)\n",
    "\n",
    "        return new_datum_pred, new_datum_truth, new_data, new_index, dict_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_cluster  truth_cluster\n",
       "0             1                699\n",
       "              0                223\n",
       "1             0                774\n",
       "              1                301\n",
       "Name: truth_cluster, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truths = [0]*1000 + [1]*1000\n",
    "new_datum_pred, new_datum_truth, new_data, new_index,dict_pred = parse_dreem_results(\"/home/han/proj_het_AC/rerun_analysis/0_Manuscript_codes_submission_20250723/Figure2/Data/DMS-MaP/result/output/ADD/K_2/run_1-best/Responsibilities.txt\", \n",
    "                                                                           2, \n",
    "                                                                           library,\n",
    "                                                                           truths,\n",
    "                                                                           p_threshold=0.5)\n",
    "\n",
    "df = pd.DataFrame(list(zip(list(new_datum_pred), list(new_datum_truth))), columns=['pred_cluster','truth_cluster'])\n",
    "df.groupby(['pred_cluster'])['truth_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
